Divide and Conquer Paradigm
1) Divide problem into smaller subproblems
 -- sometimes this is literal, sometimes more conceptual
2) Conquer subproblems using recursion
3) Combine solutions of subproblems into one for the original problem
 -- generally where most ingenuity is required

Array inversions
. given an array, an inversion is just a pair of indices (i, j) such that i<j and A[i] > A[j]
 -- if array is in sorted order --- 1,2,3,4,5 -- there are NO inversions
 -- if in reverse order, there is the most number of possible inversions
    -- number of possible inversions is quadratic relative to n
 -- for any non-perfectly-sorted array, there will be at least one inversion
. why the hell is this useful?
 -- comparing different ranked lists
 -- "collaborative filtering" -- e.g., product recommendations on Amazon website based
    on comparing you to other people with similar preferences

6,5,4,3,2,1 -- 15 possible inversions ( n
                                        2 ) = (n(n-1))/2

Possible approaches to solving this for a given array:
. Brute force -- quadratic time, though
. Divide and conquer
 -- similar to merge sort
 -- categorize inversions into 3 types -- n is length of array: 
   -- left - both i,j <= n/2; 
   -- right - both i,j >= n/2; 
   -- split - 1 of i,j <= n/2  other of i,j >= n/2
 -- splitting these into 2, left split will total left inversions, right split will total right inversions
 -- base case: if n=1 return 0
 -- if correctly done, should be O(n*log(n)) time
 -- separate subroutines for left inversions, right inversions, and splits
 -- how to solve the split inversion scenario?
   -- sort the left and right arrays, too, in that subroutine
   -- the process of sorting both left and right arrays, iterating through both and copying items into
      resulting array will identify split inversions -- any time an element of the right array is copied
      before exhausting the left array will reveal another split inversion
   -- pattern that emerges: whenever an element of the right array is copied before left is empty, there are
      at least the number-of-elements-left-in-left-array number of split inversions
. Weird shit -- O(n) + O(n) = O(n) -- but only if number of terms is constant (not n, duh)
. End result: n*log(n) algorithm

Matrix Multiplication
. Traditional iterative algorithm is O(n^3)
. Can we do better?
. Applying divide and conquer:
 -- Assuming Square matrices: Divide individual matrices into smaller matrices
   -- n/2 x n/2 matrices
 -- Recursively compute the products -- 8 recursive calls
 -- but this is also O(n^3)
. Strassen's Algorithm -- example of clever algorithm design:
 -- reduces number of recursive calls from 8 to 7, but makes huge difference
 -- weird, non-intuitive algorithm

Closest Pair algorithm -- geometry, determine closest pair of points in a plane
. distance -- d(p_i, p_j)
              = sqrt((x_i-x_j)^2 + (x_i-y_j)^2)
. assume: no ties; all points have distinct x and y coords
. Brute force approach:
 -- iterate through all points in nested for loops and remember shortest distance
 -- O(n^2)
 -- can we do better?
. Simplified problem: find closest pair on 1-dimensional plane
 1) sort points -- merge sort (O(nlog n)) time
 2) return closest pair of adjacent points (  O(n) time)
 -- observation: brute force is still quadratic here, so 1-dimensional example may be relevant

. Real problem
 1) Make 2 copies of points -- one sorted by x, other sorted by y
  -- nlog n time
 2) for each set of Points, Px, Py, divide them into left and right pair
 3) for each set of left and right points, caclulate closest pair
 4) Split points (where one of closest points is in left half and other in right)?
*** -- we actually don't need a subroutine to do this in order to get a correct algorithm
.... trailing off

Understanding running times of problems (with multiplication algorithms as examples)
. T(n) -- worst-case number of operations required to perform an algorithm
. Recurrence: expressing T(n) in terms of running times of recursive calls
 FOR GRADE SCHOOL ALGORITHM:
 -- base case: T(1) <= a constant
 -- for all n > 1: T(n) <= 4T(n/2) + linear term (O(n))
   -- in other words: within each recursive call, 4 other calls are made and some linear amount of stuff is done
 GAUSSIAN ALGORITHM:
 -- base case: T(1) <= a constant
 -- for all n > 1: T(n) <= 3T(n/2) + linear term (O(n))
 -- this is one fewer recursive calls than grade school

Master Method aka Master Theorem
" black box for solving recurrences"
. takes as input recurrence in particular format and spits out solution to it
. Assumption: all subproblems have exactly same size
 -- eg., in merge sort, subproblems operate equally on HALF of input

. Recurrence format
 -- Base case: T(n) <= a constant for all sufficiently small n
 -- For all larger n: T(n) <= aT(n/b) + O(n^d)
   -- a is number of recursive calls (or number of subproblems)
   -- n/b is fraction of the original input size (b is constant strictly >= 1)
   -- d is exponent in summing time of "combine step" -- work done outside the recursive calls
   -- a, b, and d are INDEPENDENT of n

************
. T(n) (upper) bounded by 3 cases:
1) if a = b^d -- O(n^d*log(n))  -- base of logarithm doesn't matter here
 -- work done per level is same at every level
2) if a < b^d -- O(n^d)
 -- work done per level decreases at every level; root is the "worst" level
3) if a > b^d -- O(n^(log_b(a))) -- base is very important here because it's in the exponent
 -- work per level increases at every level; leaves are the "worst" level
************

. For merge sort:
 -- a = 2 (because there are 2 recursive calls)
 -- b = 2 (because each recursive call reduces input by half)
 -- d = 1 (because merging is linear time)
 -- So: a = b^d == O(nlogn)

. Binary search
 -- like looking up a name in a phone book
 -- a = 1 (because you recurse once, check whether > or < )
 -- b = 2 (because you reduce by half each time)
 -- d = 0 (because ... I don't get why) -- outside of recursive call, only one comparison
 -- So: a = b^d == O(logn)

. Basic Integer Multiplication
 -- a = 4 
 -- b = 2
 -- d = 1 -- because addition can be done in linear time
 -- So: T(n) = O(n^(log_b*a)
             = O(n^(log_2*4)
             = O(n^2)

. Gauss Integer Multiplication
 -- a = 3
 -- b = 2 
 -- d = 1
 -- T(n) = O(n^log_2*3) = O(n^1.59)

. Strassens' matrix multiplication
 -- a = 7
 -- b = 2
 -- d = 1
 -- T(n) = O(n^2.81)
 
Proof of Master Method
. Assumptions:
 -- T(1) <= c
 -- T(n) <= aT(n/b) + cn^d
 -- n is a power b
. at each level j for merge sort
 -- a^j subproblems of size n/(b^j) 
. Recursion tree
 -- level 0: initial input
 -- level 1: a subproblems of size n/b
 -- ...
 -- level lob_b(n): base case

. Looking at work at a single level -- level j:
 -- a^j subproblems
 -- n/(b^j) size of individual subproblems
 -- work done outside of recursion : c * (n/(b^j))^d
 -- so, formula looks like this:
 <= a^j * c * (n/(b^j))^d
... separate the terms that depend on the level j (a and b) from those that are independent (d and c)
 = cn^d * (a/(b^d))^j

 -- Summing over all levels j
 *** total work <= cn^d * Sigma_j...log_b(n) [a/(b^d))^j]
... the a/(b^d) term is central to understanding the three different cases of the master method

... a = rate of subproblem proliferation (RSP), the rate at which number of subproblems grow with each level of recursion -- "evil"
... b^d = rate of work shrinkage (rws) -- "good"

More on proving master theorem
. if a/(b^d) == 1, easy to solve ... O(n^d*log(n))
. if a/(b^d) != 1 , look at 'r', the ratio of a to (b^d)
 -- imagine adding (for each iteration in the sigma) 1 + r + r^2 + r^3 + r^4 + r^5
   -- if r < 1, this sum never gets bigger than 2 ( 1/1-r)
     -- the first term in the addition (1) is dominant, because the rest of the terms added are never bigger than this
     -- independent of k
     -- applying this to the second scenario of the master method...
       a/b^d is a constant, can factor it out, so <= cn^d * Sigma_j...log_b(n) [a/(b^d))^j]
     -- becomes O(n^d)
   -- if r > 1, it tends toward infinity (1+ (1/r-1))
     -- so the last term (r^k) dominates -- all the previous terms added are still less than this term
     -- applying to third scenario of master method... 
     -- O(n^d) * (a/b^d)^(log_b(n))  -- (replace j with its largest value (since that dominates) -- log_b(n))
        = cancel out the /b^d and ^log_b(n) ... = n^-d
        = O(a^(log_b(n)))
****    = this quantity is exactly the number of LEAVES in the recursion tree 
   -- but... why did we get O(a^(log_b(n))) instead of O(n^(log_b(a))) ( which is what we originally put for the third term of master method) ????
     -- they're the exact same thing -- O(a^(log_b(n))) == O(n^(log_b(a)))

High level points of the proof
. if a = b^d, (case 1) 
  -- we know we do same amount of work at each level
  -- we know there is n^d work at each level
  -- we know there are log_n number of levels
  -- so, O(n^dlogn)
. if a < b^d (case 2)
  -- we know root level dominates
  -- we know we do n^d work at root
  -- O(n^d)
. if a > b^d (case 3)
  -- we know the leaves dominates
  -- we know there are n^(log_b(a)) leaves
  -- O(n^(log_b(a)))


