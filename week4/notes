Selection Problem
. finding the i'th smallest element of an array
 -- aka the "i'th order statistic"
. easy way: O(nlogn) algorithm
 --  REDUCTION --
 1) merge sort
 2) return ith element of sorted array
. ** can we do better?
 -- no, not by reducing to a sorting at least
. but selection can be done in linear time


Randomized Selection algorithm
. Partition around a pivot -- same as quicksort
 -- then recurse on the side where the desired statistic is,
    and look for the (i'th minus pivot index), then you're done
. only one recursive call instead of 2 (since you only
  care about the side with the desired element)
. subroutine:

RSelect(A, n, i)
  if n == 1: return A[0]
  choose pivot p from A at random
  partition A around p ('j' is the index of the pivot)
  if j == i:
    return p
  if j > i:
   RSelect(left side of A, j-1, i)
  else:
   RSelect(right side of A, n-j, i-j)

Running time of randomized selection algorithm
. worst case: O(n^2)
 ( imagine a nearly sorted array and taking the first element as the pivot 
   each time you recurse; if so, each recursion would only reduce your
   array by half )
 -- but this is unlikely to happen
. best case: O(n) -- (ie, we happen to choose median each time)
. recurrence T(n) <= T(n/2) + O(n)
  -- reducing size by half each recursion, and partitioning is done in linear time
  -- case 2 of master method
. *** this is true irrespective of the data -- no assumptions required about nature of data
. so, how to choose pivot?

Proof
. note: RSelect uses <= cn operations -- c is a constant
  representing work done outside the recursive call
. phases: indicate amount of progress made, with higher phases
  meaning more progress
 -- in phase 'j' if array size between [(3/4)^(j+1) * n] and [(3/4)^j * n]
 -- example: j = 0 (original array size = 10)
    -- j between 3/4(n) and n
    -- so, if array size on subsequent recursive calls is still >= 3/4 of n,
       you're still in phase 0
. X_j -- number of recursive calls while in phase j
. Total running time, then becomes:
   <= Sigma_xj( Xj * c * (3/4)^j * n )
  -- summing the work done in each phase for all phases
  -- both X_j and c*3/4^j*n are random variables, so we are interested
     in their Expected values
. E[Xj] <= expected # of times you need to flip a fair coin to get a head
 -- N = number of coin flips
 -- you can defined E[N] in terms of itself
 -- always will need at least 1
. E[N] = 1 + (1/2) + E[N] 
 -- 1 because you always will need at least 1 toss
 -- 1/2 because of the probability you get tails and have to flip again
 -- E[N] because once you start the next toss, your probability of 
    getting heads is the same as it was before....
. Solution: E[N] = 2 ... I don't get it

. Simplyifying the above equation for total running time....
  <= E[ cn * Sigma( (3/4)^j * Xj)
 -- you can take cn out of sigma because they're constant
. simplifying further:
  <= cn * Sigma((3/4)^j * E[Xj])
. since E[Xj] == E[N] == 2:
  <= 2cn * Sigma((3/4)^j) -- geometric sum
. 1/(1-(3/4)) = 4
*********
 <= 8cn
*********

Deterministic (as opposed to random) selection
. doesn't work as well as randomized, but is a very 
  cool algorithm
. goal: find pivot guaranteed to be pretty good
 -- key idea: use 'median of medians'
. think of it as a 2-round tournament
  -- (conceptually) break into n/5 groups of 5 elements each
  -- use merge sort to take median of each sub-array
      ( choice of sorting algorithm doesn't really matter due
        to small absolute size of arrays )
  -- take the 5 the medians from previous step and then
     get median of those

DSelect(A, n, i ) -- where i is the desired index aka 'order statistic'
  break A into groups of 5, sort each group
  c = the n/5 "middle elements"
  p = Select(C, n/5, n/10) -- recursively compute median of C
  ### same as Rselect from here on out:
  partition A around p
  if j == i:
    return p
  if j < i:
    return Select(1st part of A, j-1, i)
  else:
    return Select(2nd part of A, j-1, i)
 
About DSelect
. 2 recursive calls, not just one like in RSelect
 -- 1st one determines pivot, 2nd one 
. will never take O(n^2) time like RSelect
  -- still linear even in worst case
. but it's not as efficient as RSelect in practice
 1) larger constants hidden by big oh notation
 2) requires extra memory to hold "1st-round" winners



