Selection Problem
. finding the i'th smallest element of an array
 -- aka the "i'th order statistic"
. easy way: O(nlogn) algorithm
 --  REDUCTION --
 1) merge sort
 2) return ith element of sorted array
. ** can we do better?
 -- no, not by reducing to a sorting at least
. but selection can be done in linear time


Randomized Selection algorithm
. Partition around a pivot -- same as quicksort
 -- then recurse on the side where the desired statistic is,
    and look for the (i'th minus pivot index), then you're done
. only one recursive call instead of 2 (since you only
  care about the side with the desired element)
. subroutine:

RSelect(A, n, i)
  if n == 1: return A[0]
  choose pivot p from A at random
  partition A around p ('j' is the index of the pivot)
  if j == i:
    return p
  if j > i:
   RSelect(left side of A, j-1, i)
  else:
   RSelect(right side of A, n-j, i-j)

Running time of randomized selection algorithm
. worst case: O(n^2)
 ( imagine a nearly sorted array and taking the first element as the pivot 
   each time you recurse; if so, each recursion would only reduce your
   array by half )
 -- but this is unlikely to happen
. best case: O(n) -- (ie, we happen to choose median each time)
. recurrence T(n) <= T(n/2) + O(n)
  -- reducing size by half each recursion, and partitioning is done in linear time
  -- case 2 of master method
. *** this is true irrespective of the data -- no assumptions required about nature of data
. so, how to choose pivot?

Proof
. note: RSelect uses <= cn operations -- c is a constant
  representing work done outside the recursive call
. phases: indicate amount of progress made, with higher phases
  meaning more progress
 -- in phase 'j' if array size between [(3/4)^(j+1) * n] and [(3/4)^j * n]
 -- example: j = 0 (original array size = 10)
    -- j between 3/4(n) and n
    -- so, if array size on subsequent recursive calls is still >= 3/4 of n,
       you're still in phase 0
. X_j -- number of recursive calls while in phase j
. Total running time, then becomes:
   <= Sigma_xj( Xj * c * (3/4)^j * n )
  -- summing the work done in each phase for all phases
  -- both X_j and c*3/4^j*n are random variables, so we are interested
     in their Expected values
. E[Xj] <= expected # of times you need to flip a fair coin to get a head
 -- N = number of coin flips
 -- you can defined E[N] in terms of itself
 -- always will need at least 1
. E[N] = 1 + (1/2) + E[N] 
 -- 1 because you always will need at least 1 toss
 -- 1/2 because of the probability you get tails and have to flip again
 -- E[N] because once you start the next toss, your probability of 
    getting heads is the same as it was before....
. Solution: E[N] = 2 ... I don't get it

. Simplyifying the above equation for total running time....
  <= E[ cn * Sigma( (3/4)^j * Xj)
 -- you can take cn out of sigma because they're constant
. simplifying further:
  <= cn * Sigma((3/4)^j * E[Xj])
. since E[Xj] == E[N] == 2:
  <= 2cn * Sigma((3/4)^j) -- geometric sum
. 1/(1-(3/4)) = 4
*********
 <= 8cn
*********

Deterministic (as opposed to random) selection
. doesn't work as well as randomized, but is a very 
  cool algorithm
. goal: find pivot guaranteed to be pretty good
 -- key idea: use 'median of medians'
. think of it as a 2-round tournament
  -- (conceptually) break into n/5 groups of 5 elements each
  -- use merge sort to take median of each sub-array
      ( choice of sorting algorithm doesn't really matter due
        to small absolute size of arrays )
  -- take the 5 the medians from previous step and then
     get median of those

DSelect(A, n, i ) -- where i is the desired index aka 'order statistic'
  break A into groups of 5, sort each group
  c = the n/5 "middle elements"
  p = Select(C, n/5, n/10) -- recursively compute median of C
  ### same as Rselect from here on out:
  partition A around p
  if j == i:
    return p
  if j < i:
    return Select(1st part of A, j-1, i)
  else:
    return Select(2nd part of A, j-1, i)
 
About DSelect
. 2 recursive calls, not just one like in RSelect
 -- 1st one determines pivot, 2nd one 
 -- even though 1st call does recursive sorting,
    it still works in linear time because the
    small group size... 
 -- because the pivot is the median of medians
    in 1st recursive call, it's guaranteed to 
    reduce the array size by at least 30%
. will never take O(n^2) time like RSelect
  -- still linear even in worst case
. but it's not as efficient as RSelect in practice
 1) larger constants hidden by big oh notation
 2) requires extra memory to hold "1st-round" winners

Rough Outline of Recurrence of DSelect
. T(n) = maximum running time of Dselect on array of length n
. Base case:
  -- T(1) = 1
  -- T(n) <= cn + T(n/5) + T(?)
    -- cn is the sorting/partition
    -- T(n/5) is the initial recursive call
    -- T(?) because we never know exactly how much work will be done in 2nd
       recursive call b/c we don't know in advance exactly
       how good of a choice our pivot will be
      -- (but, we know it will trim off at least 30%, so 30/70 split or better)

But how prove that DSelect is O(n)
. can't use master method because the subproblems are different sizes
. strategy: "hope and check"
. hope: there is some constant a such that T(n) <= an 
. let a = 10c -- ok...
. ....


Graphs
. two ingredients
 -- vertices aka nodes (V = set of vertices)
 -- edges (E) == pairs of vertices
. directed vs undirected graphs
 -- edges can be undirected -- unordered pairs
 -- or directed -- ie, 1st and 2nd, sometimes known as arcs
   -- arrows indicate directed edges, point to 2nd vertex of edge
   -- parallel arcs are where two points have two ordered lines
      between them
. example uses:
 -- road networks, in google maps, stored as graphs
 -- the web itself
   -- vertices: web pages
   -- edges: hyperlinks between two pages
 -- social networks (some directed, some undirected)
 -- representing dependencies
. cuts of graphs
 -- a cut is a partition of a graph into 2 graphs
 -- partitions can be undirected or directed, the
    edges can be entirely within a partition or 
    extend across them (aka crossing edges)
 -- with directed cuts, you typically only 
    think about cuts that cross from LEFT to RIGHT;
    RIGHT to LEFT usually ignored
 -- approx 2^n cuts per graph, since each vertex
    can go into one cut or the other
. minimum cut problem:
 -- find cut with fewest number of crossing edges
 -- applications:
   -- identify weaknesses in a network (eg, a computer network)
   -- community detection (ie, in a social network)
   -- image segmentation in computer vision
     -- input = 2d array of pixels; edges are between neighboring pixels
        -- weight of an edge is likelihood of neighboring pixels coming from
           same object, allowing you to identify objects in a picture
. n = number of vertices
. m = number of edges

Sparse vs Dense Graphs
. min and max number of edges of graphs assuming
  "connected" (one piece) and no parallel arcs:
 -- min: n - 1
 -- max: n(n-1)/2
. "sparse" is closer to min: O(n)
. "dense" is closer to max: O(n^2)

Adjacency matrix
. Represent edges of graph using a matrix
. A -- nxn matrix
. A_ij = 1 if edge between i and j
  -- or more than if parallel edges
  -- or +1 or -1 to account for directed edges
. space requirement to represent this: Theta(n^2)
 -- independent of number of edges 

Adjacency list
 -- arry of vertices
 -- array of edges
  -- each edge points to it endpoints
  -- each vertex points to edges incident on it
. space requirement: Theta(m+n)

Which is better: matrices or lists?
. Depends on graph density


Random Contraction Algorithm
. basically, the algorithm randomly identifies an edge, 'contracts'
  it, which turns two vertices into a single 'super-vertex',
  merges the jumbled eges, deleting 'self-loops', and arrives
  at a result that is hopefully a minimum cut, ie, results in
  the smallest number of edges that cross
. sometimes identifies min cut, sometimes doesn't
. algorithm:
  while > 2 vertices:
    pick a remaining edge (u,v) at random
    merge (or "contract") u and v into a single vertex
    remove self-loops
  return cut represented by final 2 vertices

What is the probability of getting a Minimum Cut with Random Contraction?
. graph = G(V, E) with n vertices, m edges
. Minimum cut (A,B) -- the two groups of vertices chosen
. key issue is whether an edge that crosses (A,B) is chosen at random;
  this will break shit
. F is the edges that cross -- those we don't want to choose
. k is the NUMBER of edges that cross
. Proability(output is (A,B)) = Pr( never contract an edge of F)
. S_i -- choosing an edge of F
. Goal: compute Pr(each iteration doesn't yield S_i)
. Probability of never getting an edge in F becomes the conditional probability
  of each individual iteration
  (1-2/n) * (1-2(n-1)) * (1-2/(n-2)) .... = cancelling out... 2/(n(n-1)) ... 
   -- sloppy: 1/n^2
   -- this is a low probability of never getting an edge in F
   -- but... repeated trials can make this probability much higher
. Solution: run basic algorithm a large number of times, remember the smallest cut found
 -- T_i = event that the cut (A,b) is found on the i'th try
. Running time: not exponential, but slow, polynomial



