Selection Problem
. finding the i'th smallest element of an array
 -- aka the "i'th order statistic"
. easy way: O(nlogn) algorithm
 --  REDUCTION --
 1) merge sort
 2) return ith element of sorted array
. ** can we do better?
 -- no, not by reducing to a sorting at least
. but selection can be done in linear time


Randomized Selection algorithm
. Partition around a pivot -- same as quicksort
 -- then recurse on the side where the desired statistic is,
    and look for the (i'th minus pivot index), then you're done
. only one recursive call instead of 2 (since you only
  care about the side with the desired element)
. subroutine:

RSelect(A, n, i)
  if n == 1: return A[0]
  choose pivot p from A at random
  partition A around p ('j' is the index of the pivot)
  if j == i:
    return p
  if j > i:
   RSelect(left side of A, j-1, i)
  else:
   RSelect(right side of A, n-j, i-j)

Running time of randomized selection algorithm
. worst case: O(n^2)
 ( imagine a nearly sorted array and taking the first element as the pivot 
   each time you recurse; if so, each recursion would only reduce your
   array by half )
 -- but this is unlikely to happen
. best case: O(n) -- (ie, we happen to choose median each time)
. recurrence T(n) <= T(n/2) + O(n)
  -- reducing size by half each recursion, and partitioning is done in linear time
  -- case 2 of master method
. *** this is true irrespective of the data -- no assumptions required about nature of data
. so, how to choose pivot?

Proof
. note: RSelect uses <= cn operations -- c is a constant
  representing work done outside the recursive call
. phases: indicate amount of progress made, with higher phases
  meaning more progress
 -- in phase 'j' if array size between [(3/4)^(j+1) * n] and [(3/4)^j * n]
 -- example: j = 0 (original array size = 10)
    -- j between 3/4(n) and n
    -- so, if array size on subsequent recursive calls is still >= 3/4 of n,
       you're still in phase 0
. X_j -- number of recursive calls while in phase j
. Total running time, then becomes:
   <= Sigma_xj( Xj * c * (3/4)^j * n )
  -- summing the work done in each phase for all phases
  -- both X_j and c*3/4^j*n are random variables, so we are interested
     in their Expected values
. E[Xj] <= expected # of times you need to flip a fair coin to get a head
 -- N = number of coin flips
 -- you can defined E[N] in terms of itself
 -- always will need at least 1
. E[N] = 1 + (1/2) + E[N] 
 -- 1 because you always will need at least 1 toss
 -- 1/2 because of the probability you get tails and have to flip again
 -- E[N] because once you start the next toss, your probability of 
    getting heads is the same as it was before....
. Solution: E[N] = 2 ... I don't get it

. Simplyifying the above equation for total running time....
  <= E[ cn * Sigma( (3/4)^j * Xj)
 -- you can take cn out of sigma because they're constant
. simplifying further:
  <= cn * Sigma((3/4)^j * E[Xj])
. since E[Xj] == E[N] == 2:
  <= 2cn * Sigma((3/4)^j) -- geometric sum
. 1/(1-(3/4)) = 4
*********
 <= 8cn
*********

Deterministic (as opposed to random) selection
. doesn't work as well as randomized, but is a very 
  cool algorithm
. goal: find pivot guaranteed to be pretty good
 -- key idea: use 'median of medians'
. think of it as a 2-round tournament
  -- (conceptually) break into n/5 groups of 5 elements each
  -- use merge sort to take median of each sub-array
      ( choice of sorting algorithm doesn't really matter due
        to small absolute size of arrays )
  -- take the 5 the medians from previous step and then
     get median of those

DSelect(A, n, i ) -- where i is the desired index aka 'order statistic'
  break A into groups of 5, sort each group
  c = the n/5 "middle elements"
  p = Select(C, n/5, n/10) -- recursively compute median of C
  ### same as Rselect from here on out:
  partition A around p
  if j == i:
    return p
  if j < i:
    return Select(1st part of A, j-1, i)
  else:
    return Select(2nd part of A, j-1, i)
 
About DSelect
. 2 recursive calls, not just one like in RSelect
 -- 1st one determines pivot, 2nd one 
 -- even though 1st call does recursive sorting,
    it still works in linear time because the
    small group size... 
 -- because the pivot is the median of medians
    in 1st recursive call, it's guaranteed to 
    reduce the array size by at least 30%
. will never take O(n^2) time like RSelect
  -- still linear even in worst case
. but it's not as efficient as RSelect in practice
 1) larger constants hidden by big oh notation
 2) requires extra memory to hold "1st-round" winners

Rough Outline of Recurrence of DSelect
. T(n) = maximum running time of Dselect on array of length n
. Base case:
  -- T(1) = 1
  -- T(n) <= cn + T(n/5) + T(?)
    -- cn is the sorting/partition
    -- T(n/5) is the initial recursive call
    -- T(?) because we never know exactly how much work will be done in 2nd
       recursive call b/c we don't know in advance exactly
       how good of a choice our pivot will be
      -- (but, we know it will trim off at least 30%, so 30/70 split or better)

But how prove that DSelect is O(n)
. can't use master method because the subproblems are different sizes
. strategy: "hope and check"
. hope: there is some constant a such that T(n) <= an 
. let a = 10c -- ok...
. ....


Graphs
. two ingredients
 -- vertices aka nodes (V = set of vertices)
 -- edges (E) == pairs of vertices
. directed vs undirected graphs
 -- edges can be undirected -- unordered pairs
 -- or directed -- ie, 1st and 2nd, sometimes known as arcs
   -- arrows indicate directed edges, point to 2nd vertex of edge
   -- parallel arcs are where two points have two ordered lines
      between them
. example uses:
 -- road networks, in google maps, stored as graphs
 -- the web itself
   -- vertices: web pages
   -- edges: hyperlinks between two pages
 -- social networks (some directed, some undirected)
 -- representing dependencies
. cuts of graphs
 -- a cut is a partition of a graph into 2 graphs
 -- partitions can be undirected or directed, the
    edges can be entirely within a partition or 
    extend across them (aka crossing edges)
 -- with directed cuts, you typically only 
    think about cuts that cross from LEFT to RIGHT;
    RIGHT to LEFT usually ignored
 -- approx 2^n cuts per graph, since each vertex
    can go into one cut or the other
. minimum cut problem:
 -- find cut with fewest number of crossing edges
 -- applications:
   -- identify weaknesses in a network (eg, a computer network)
   -- community detection (ie, in a social network)
   -- image segmentation in computer vision
     -- input = 2d array of pixels; edges are between neighboring pixels
        -- weight of an edge is likelihood of neighboring pixels coming from
           same object, allowing you to identify objects in a picture
. n = number of vertices
. m = number of edges

Sparse vs Dense Graphs
. min and max number of edges of graphs assuming
  "connected" (one piece) and no parallel arcs:
 -- min: n - 1
 -- max: n(n-1)/2
. "sparse" is closer to min: O(n)
. "dense" is closer to max: O(n^2)

Adjacency matrix
. Represent edges of graph using a matrix
. A -- nxn matrix
. A_ij = 1 if edge between i and j
  -- or more than if parallel edges
  -- or +1 or -1 to account for directed edges
. space requirement to represent this: Theta(n^2)
 -- independent of number of edges 

Adjacency list
 -- arry of vertices
 -- array of edges
  -- each edge points to it endpoints
  -- each vertex points to edges incident on it
. space requirement: Theta(m+n)

Which is better: matrices or lists?
. Depends on graph density


Random Contraction Algorithm
. basically, the algorithm randomly identifies an edge, 'contracts'
  it, which turns two vertices into a single 'super-vertex',
  merges the jumbled eges, deleting 'self-loops', and arrives
  at a result that is hopefully a minimum cut, ie, results in
  the smallest number of edges that cross
. sometimes identifies min cut, sometimes doesn't
. algorithm:
  while > 2 vertices:
    pick a remaining edge (u,v) at random
    merge (or "contract") u and v into a single vertex
    remove self-loops
  return cut represented by final 2 vertices

What is the probability of getting a Minimum Cut with Random Contraction?
. graph = G(V, E) with n vertices, m edges
. Minimum cut (A,B) -- the two groups of vertices chosen
. key issue is whether an edge that crosses (A,B) is chosen at random;
  this will break shit
. F is the edges that cross -- those we don't want to choose
. k is the NUMBER of edges that cross
. Proability(output is (A,B)) = Pr( never contract an edge of F)
. S_i -- choosing an edge of F
. Goal: compute Pr(each iteration doesn't yield S_i)
. Probability of never getting an edge in F becomes the conditional probability
  of each individual iteration
  (1-2/n) * (1-2(n-1)) * (1-2/(n-2)) .... = cancelling out... 2/(n(n-1)) ... 
   -- sloppy: 1/n^2
   -- this is a low probability of never getting an edge in F
   -- but... repeated trials can make this probability much higher
. Solution: run basic algorithm a large number of times, remember the smallest cut found
 -- T_i = event that the cut (A,b) is found on the i'th try
. Running time: not exponential, but slow, polynomial

Number of Minimum Cuts
. a __tree__ has n-1 different minimum cuts
. for a graph:
 -- the largest possible number of minimum cuts: n choose 2 == (n(n-1))/2



3. 
Let .5<α<1 be some constant. Suppose you are looking for the median element in an array using RANDOMIZED SELECT (as explained in lectures). What is the probability that after the first iteration the size of the subarray in which the element you are looking for lies is ≤α times the size of the original array?

1−α/2 -- not this

2*α - 1 -- I think it's this

α−1/2 -- not this

1−α -- not this


4. 
Let 0<α<1 be a constant, independent of n. Consider an execution of RSelect in which you always manage to throw out at least a 1−α fraction of the remaining elements before you recurse. What is the maximum number of recursive calls you'll make before terminating?

−log(n)log(α)

−log(n)α

−log(n)log(12+α)

−log(n)/ log(1−α) -- I think this is correct
1
point

5. 
The minimum s-t cut problem is the following. The input is an undirected graph, and two distinct vertices of the graph are labelled "s" and "t". The goal is to compute the minimum cut (i.e., fewest number of crossing edges) that satisfies the property that s and t are on different sides of the cut.

Suppose someone gives you a subroutine for this s-t minimum cut problem via an API. Your job is to solve the original minimum cut problem (the one discussed in the lectures), when all you can do is invoke the given min s-t cut subroutine. (That is, the goal is to reduce the min cut problem to the min s-t cut problem.)

Now suppose you are given an instance of the minimum cut problem -- that is, you are given an undirected graph (with no specially labelled vertices) and need to compute the minimum cut. What is the minimum number of times that you need to call the given min s-t cut subroutine to guarantee that you'll find a min cut of the given graph?

n -- not this

(n choose 2) -- not this

2n

n−1


##########################################
Let "output" denote the cut output by Karger's min cut algorithm on a given connected graph with n vertices, and let p=1(n2). Which of the following statements are true?

For every graph G with n nodes and every min cut (A,B),

Pr[out=(A,B)]≤p. Un-selected is correct 

For every graph G with n nodes, there exists a min cut (A,B) of G such that

Pr[out=(A,B)]≥p. This should be selected 

There exists a graph G with n nodes and a min cut (A,B) of G such that

Pr[out=(A,B)]≤p. This should be selected 
An n-cycle works.

For every graph G with n nodes and every min cut (A,B) of G,

Pr[out=(A,B)]≥p. This should be selected 
This is exactly what we proved in lecture.

For every graph G with n nodes, there exists a min cut (A,B) such that

Pr[out=(A,B)]≤p. Un-selected is correct 
##############################











