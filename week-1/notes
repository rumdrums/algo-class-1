---
** fundamental question for every algorithm designer: CAN WE DO BETTER? **
Integer multiplication
. Input
 -- two n-digit numbers x and y
. Output
 -- the product x*y
. The usual approach to solving multiplication by hand requires roughly 2n^2 operations, 
  each increase of n results in factor of 2 increase in number of operations (quadratic)

. but, Karatsuba multiplication... 
 -- x*y = 10^n*ac + 10^(n/2)(ad+bc) + bd
 -- a,b and c,d are result of dividing x and y, respectively, into smaller numbers
 -- come up with recursive algorithm for this

Merge sort
. Intro to divide and conquer ( divide into smaller parts, solve recursively)
. Improves over selection, insertion, and bubble sorts
 -- all of these scale with n^2 as n increases
. Input: array of n numbers in arbitrary order
. Output: sorted array

. Input: [5,4,1,8,7,2,6,3]
 -- break into two arrays: [5,4,1,8] and [7,2,6,3]
 -- recurse
 -- merge

. Efficiency:
 -- given n, merge sort requires at most 6n * log_2(n) + 6n operations
 -- each level of recursion reduces by factor of 2 the input size (e.g., 16 -> 8, 4, 2, 1)
 -- number of steps: (log_2(n) + 1)
 -- for recursion level j=0,1,2,3,..., log_2(n)
    -- at level j there are 2^j subproblems, and each subproblem has n/(2^j) inputs
 -- to caclulate amount of work done at each level of j:
    -- 2^j subproces * 6*n/2^j inputs ... 2^j cancels out, left with 6n -- independent of j
    -- so, total amount of work is 6n * ( log_2(n) + 1 )

Guiding Principles:
1) Worst-case analysis
 -- i.e., equal consideration of any size input
 -- no consideration of "likely" or "average" input -- calculation that holds true for any input n
2) Don't pay much attention to constant factors -- eg, number of operations at a single level of analysis (e.g., 6n vs 4n)
 -- easier
 -- number of constant factors are likley to vary based on implementation / compiler anyway
 -- lose very little
3) Asymptotic analysis 
 -- performance of algorithm at large values of n
 -- Justification: only big problems are interesting
 -- even with Moore's law, this still holds, because problems get more ambitious as processing speed increases

What is a fast algorithm?
. Worst-case running time grows slowly with input size
. goal is linear time -- not always possible


Asymptotic Analysis
. Identifying the "sweet spot" for high-level reasoning about algorithms
. Supress constant factors (too system-dependent) and lower-order terms (irrelevant for large inputs)
 -- e.g., 6n*log_2(n) + 6n
    -- first 6 is the constant factor, +6n is the lower-order term
    -- so, reduce to n*log_2(n)
    -- running time is O (big oh) (n*log_2(n))

. Example: searching through array for a specific number (iterate through, return True once found, false if hit end of array)
 -- O(n) -- linear time
. Example 2: searching through 2 arrays for a specific number 
 -- still O(n) -- approximately twice as many iterations required, but the 2 is just a leading factor that can be eliminated
. Example 3: searching through 2 nested arrays for a number in common
 -- O(n^2) -- quadratic time

Big Oh Notation Definition
. T(n) = O(f(n)), n=1,2,3,...
 -- ... meaning, for sufficiently large values of n, T(n) is bounded above by a constant multiple of f(n)
. Formal definition:
 -- Given constant, c, n_0 > 0: T(n) <= c * f(n) for all n >= n_0
 -- c and n_0 are completely independent on n
   -- c is the constant multiple of f(n)
   -- n_0 quantifies "sufficiently large"
. *** Key question to ask on a project: Does our n ever get bigger than the relevant value of n_0?


More Examples
. *** Prove that: if T(n) = a_k*n^k + ... + a_1*n + a_0, then T(n) = O(n^k)
 -- to solve: choose n_0 = 1, c = sum(abs(coefficients))
. Prove that: for every k>=1, n^k is not O(n^(k-1))
 -- to solve: prove by contradiction, n^k <= c*n^(k-1)
 -- factor out n^(k-1), you're left with n <= c
    -- if this were true, then for all sufficiently large values
      of n were, n would be less than or equal to c
    -- this is possible, so proof by contradiction






